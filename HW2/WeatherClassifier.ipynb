{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs5Q0DxG7ZRgMNTqBTIyxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doronschwartz/NLP/blob/main/HW2/WeatherClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in the Data"
      ],
      "metadata": {
        "id": "0Tju0_1-9Kiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
      ],
      "metadata": {
        "id": "qqjZJj7Z9GxM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive and load the DataSet of pictures"
      ],
      "metadata": {
        "id": "lNyR8HLeYXQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI90ma1F9DQa",
        "outputId": "aedcfa03-d434-403c-ef94-8e96c40a085c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the dataset folder\n",
        "dataset_path = '/content/drive/My Drive/dataset2'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move all the files to either Train/Test folders"
      ],
      "metadata": {
        "id": "w76Jz24QYbbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Function to move images to train or test directories\n",
        "def move_images(dataset_path, train_percent):\n",
        "    # Create train and test directories\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    test_dir = os.path.join(dataset_path, 'test')\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Get all image files in the dataset directory\n",
        "    image_files = [file for file in os.listdir(dataset_path) if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png')]\n",
        "\n",
        "    # Calculate the number of images for the train and test sets\n",
        "    num_train = int(len(image_files) * train_percent)\n",
        "    num_test = len(image_files) - num_train\n",
        "\n",
        "    # Randomly shuffle the image files\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Move images to train or test directories based on the specified percentage\n",
        "    for i, filename in enumerate(image_files):\n",
        "        if i < num_train:\n",
        "            dest_dir = train_dir\n",
        "        else:\n",
        "            dest_dir = test_dir\n",
        "\n",
        "        src_path = os.path.join(dataset_path, filename)\n",
        "        dest_path = os.path.join(dest_dir, filename)\n",
        "        shutil.move(src_path, dest_path)\n",
        "\n",
        "# Specify the dataset path and train percentage\n",
        "dataset_path = '/content/drive/My Drive/dataset2'\n",
        "train_percent = 0.8  # Percentage of images for training set\n",
        "\n",
        "# Move images to train or test directories\n",
        "move_images(dataset_path, train_percent)\n"
      ],
      "metadata": {
        "id": "qpR0WvIONzhy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create subfolders for Cloudy, Rain, Shine, Sunrise"
      ],
      "metadata": {
        "id": "01G0ner3bLKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/My Drive/dataset2/train'\n",
        "test_path = '/content/drive/My Drive/dataset2/test'\n",
        "\n",
        "def move_images_to_folders(dataset_path):\n",
        "    # Iterate over each image file in the dataset directory\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        # Extract label from the filename (alphabetic part)\n",
        "        label = re.match(r'[a-zA-Z]+', filename).group().lower()\n",
        "\n",
        "        # Create the label directory if it doesn't exist\n",
        "        label_dir = os.path.join(dataset_path, label)\n",
        "        os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "        # Move the image file to the label directory\n",
        "        src_path = os.path.join(dataset_path, filename)\n",
        "        dest_path = os.path.join(label_dir, filename)\n",
        "        shutil.move(src_path, dest_path)\n",
        "\n",
        "# Specify the dataset directory path\n",
        "dataset_path = '/content/drive/My Drive/dataset2'\n",
        "\n",
        "# Move images into folders based on the alphabetic part of the filename\n",
        "move_images_to_folders(train_path)\n",
        "move_images_to_folders(test_path)\n"
      ],
      "metadata": {
        "id": "gJN8CutCWN02"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for resetting folders"
      ],
      "metadata": {
        "id": "-JXdKs86bRr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# # Function to move images to parent directory\n",
        "# def move_images_to_parent(train_dir, test_dir):\n",
        "#     # Iterate over each image file in train and test directories\n",
        "#     for root_dir in [train_dir, test_dir]:\n",
        "#         for root, _, files in os.walk(root_dir):\n",
        "#             for file in files:\n",
        "#                 src_path = os.path.join(root, file)\n",
        "#                 dest_path = os.path.join(dataset_path, file)\n",
        "#                 shutil.move(src_path, dest_path)\n",
        "#         # Remove the train/test directory\n",
        "#         shutil.rmtree(root_dir)\n",
        "\n",
        "# # Specify the dataset path\n",
        "# dataset_path = '/content/drive/My Drive/dataset2'\n",
        "\n",
        "# # Path to train and test directories\n",
        "# train_dir = os.path.join(dataset_path, 'train')\n",
        "# test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "# # Move images to parent directory\n",
        "# move_images_to_parent(train_dir, test_dir)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "oO7lqpmOFAlO",
        "outputId": "456651fb-5a9b-496f-dc0d-ae672040dd3d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/dataset2/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-a6f3f4203e32>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Move images to parent directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmove_images_to_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-a6f3f4203e32>\u001b[0m in \u001b[0;36mmove_images_to_parent\u001b[0;34m(train_dir, test_dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Remove the train/test directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Specify the dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/dataset2/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the images and batches of photos"
      ],
      "metadata": {
        "id": "yrZLxowabU5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create ImageDataGenerator for data augmentation and preprocessing\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Load and preprocess images from the train directory\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/drive/My Drive/dataset2/train',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load and preprocess images from the test directory\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    '/content/drive/My Drive/dataset2/test',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMoCRiDM7GIp",
        "outputId": "9544a142-232d-47bd-c5ff-0ddbee967e31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 900 images belonging to 4 classes.\n",
            "Found 225 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the CNN model and retrieve the results"
      ],
      "metadata": {
        "id": "eDyIlWlJbYwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmyaf--8Du2e",
        "outputId": "045fee69-59e5-40aa-a56c-282dfdc0f5e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "28/28 [==============================] - 357s 13s/step - loss: 0.9742 - accuracy: 0.6728 - val_loss: 0.4450 - val_accuracy: 0.8839\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 117s 4s/step - loss: 0.5034 - accuracy: 0.8203 - val_loss: 0.3957 - val_accuracy: 0.8795\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 114s 4s/step - loss: 0.3727 - accuracy: 0.8664 - val_loss: 0.2422 - val_accuracy: 0.9196\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 113s 4s/step - loss: 0.2740 - accuracy: 0.8940 - val_loss: 0.3448 - val_accuracy: 0.9018\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 122s 4s/step - loss: 0.2264 - accuracy: 0.8963 - val_loss: 0.2860 - val_accuracy: 0.8884\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 112s 4s/step - loss: 0.2080 - accuracy: 0.9205 - val_loss: 0.3080 - val_accuracy: 0.9062\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 113s 4s/step - loss: 0.1457 - accuracy: 0.9435 - val_loss: 0.1622 - val_accuracy: 0.9330\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 112s 4s/step - loss: 0.1236 - accuracy: 0.9516 - val_loss: 0.4261 - val_accuracy: 0.8705\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 115s 4s/step - loss: 0.1657 - accuracy: 0.9470 - val_loss: 0.2926 - val_accuracy: 0.9062\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 114s 4s/step - loss: 0.0796 - accuracy: 0.9712 - val_loss: 0.1760 - val_accuracy: 0.9375\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1752 - accuracy: 0.9378\n",
            "Test Loss: 0.17520330846309662\n",
            "Test Accuracy: 0.9377777576446533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for the model is 97.12 and the test accuraacy is 93.75, very good results."
      ],
      "metadata": {
        "id": "sR66Bip2beKZ"
      }
    }
  ]
}